<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Policy Gradients for Aoe2 Unit Micro | Knut Peterson </title> <meta name="author" content="Knut Peterson"> <meta name="description" content="Fun little reinforcement learning project"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://knutp.github.io/projects/3_project"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Knut</span> Peterson </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Policy Gradients for Aoe2 Unit Micro</h1> <p class="post-description">Fun little reinforcement learning project</p> </header> <article> # REINFORCE Policy Gradients for AoE2 Unit Micro Fun little project I did for a reinforcement learning class in 2021. It implements the REINFORCE algorithm to micro crossbowmen in the RTS game Age of Empires 2. Below is a summary of the project. ## Background Info Age of Empires 2 (AoE2) is a popular Real-Time Strategy game where unit control can have a major impact on the game. For instance, in battles between crossbow units, arrows can be dodged, and enemy units can be focus-fired to get cost-effective trades against much larger groups of opposing units. Arrow dodging is a difficult skill, but can swing engagements and is a crucial skill for good players. The goal of this project is to answer the question: **Can we create a reinforcement learning agent that can learn to micro like a pro?** Specifically, can it learn to dodge arrow shots and win engagements with fewer units? To test this, and emulate how pro players micro, the agent must follow two limitations: <ol> <li>Controls all units as a group</li> <li>Cap on Actions Per Minute (APM) of 120</li> </ol> ## Methodology AoE2 doesn't have an automated interface to easily control a player via code, so I built a simple custom one in aoe2_game.py using pyautogui. I also created a custom scenario with 6 crossbow units per player in a confined space, and gave the RL agent 89 possible actions to choose from, where 1-88 were to click on a different grid cell, and 89 was do nothing. I based the reward function on the in-game score since it was possible to check changes in score lead by grabbing pixel colors from the in-game interface, and I assigned a reward of +5 for taking the score lead, and -1 for losing it. <figure> <img src="assets/img/raw_img2.png" width="50%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption width="40%" style="text-align: center;"> Raw image capture </figcaption> </figure> <figure> <img src="assets/img/frame_diff2.png" width="50%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption width="40%" style="text-align: center;"> Processed difference frame </figcaption> </figure> For the input of my model, I wanted to capture movement information while still keeping the input simple, so I took a tip from Andrej Karpathy’s <a href="http://karpathy.github.io/2016/05/31/rl/" rel="external nofollow noopener" target="_blank">“Pong from Pixels” article</a> on policy gradients, and calculated a difference frame to use as the input. For this I first captured 3 frames of gameplay, downsized each frame, and then calculated the difference frame. Then I finally flattened the image so the final input to the model was a list of 49686 integers representing the difference frame image. For my model, I implemented a simple multi-layer perceptron policy gradient, using the REINFORCE policy gradient algorithm. REINFORCE is a “Monte-Carlo Policy Gradient” approach, so it is relatively simple to implement, and it updates every episode. This was important for me, as it means the main computation is done AFTER the CPU is no longer being used for running the game. For my policy MLP I used an input layer of 49686 nodes (image size), and an output layer of 89 nodes (action space size). I initially only included 1 hidden layer of size 780, but testing showed that the model struggled to learn and I expanded to 2 hidden layers for the final model. <figure> <img src="assets/img/REINFORCE.png" width="50%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption width="50%" style="text-align: center;"> An overview of the REINFORCE algorithm </figcaption> </figure> ## Results The model very quickly learned that killing an enemy unit led to a reward, as shown in the demo after 20 training iterations. However, it took much longer to learn to avoid enemy arrow fire. <figure> <img src="assets/img/after-20.gif" width="40%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption width="50%" style="text-align: center;"> Initial model performance after 20 iterations </figcaption> </figure> By the end of training the model learned to focus enemy units and avoid arrow fire, but was not able to completely beat the in-game AI. <figure> <img src="assets/img/after-800.gif" width="40%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption width="50%" style="text-align: center;"> Final model performance after 800 iterations </figcaption> </figure> From the plot of the average reward vs. training episodes, we can see that the reward quickly increases as the model learns to attack enemy units, and then drops as the model learns to dodge arrows. Then the reward begins to climb again as it becomes more effective at dodging and eliminates more enemy units before losing. <figure> <img src="assets/img/avg_reward.png" width="30%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption width="50%" style="text-align: center;"> Plot of reward vs. training iterations </figcaption> </figure> While the RL agent did not manage to completely beat the in-game AI in the end, there are a few reasons for that. For one, the in-game AI does not micro like a human - it controls units individually instead of as a group, which makes it notoriously annoying to micro against. Shot dodging is also more difficult, as multiple shots may come in at the same time, while a human player would use a single group of units, making shots easier to dodge. Additionally, the other main limiting factor was the number of training episodes. Each training episode was longe and computationally intensive, as both the Aoe2 game and RL agent had to run on the same machine. As the game only ran at a set speed, this meant that 1 episode took ~1 minute of real time, and training all 800 episodes took ~13 hours spread over a total of 5 days. This could be improved by expanding to an Actor-Critic policy gradient method, or by exploring model-based methods with prioritized replay. In the end, the final agent was still very effective at dodging arrows, even against multiple units. While not quite reaching the pro level, it made a lot of progress but was limited by the number of training episodes. Overall I'm pretty happy with the results, and may revisit this at some point to try and improve it. </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Knut Peterson. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>